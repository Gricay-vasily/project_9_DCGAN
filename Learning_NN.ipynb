{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gricay-vasily/project_9_DCGAN/blob/main/Learning_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HAnq6_2XEgT"
      },
      "source": [
        "# Навчання Deep Convolutional Generative Adversarial Network (DCGAN) на завантажених даних Cifar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-aFyxljOAVZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices(\"GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "         print(\"Found a GPU with the name:\", gpu)\n",
        "else:\n",
        "    print(\"Failed to detect a GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRJnfY5sXEgT"
      },
      "source": [
        "## Завантаження і підготовка Дата-сетів"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8u2Eh9BXEgW"
      },
      "source": [
        "### Зчитування мета-даних"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvSte4oDY5F7"
      },
      "source": [
        "Посилання на дані"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc2nNalgYnNj"
      },
      "outputs": [],
      "source": [
        "URL_DIR = \"https://www.cs.toronto.edu/~kriz/\"\n",
        "cifar=10\n",
        "\n",
        "if cifar == 10:\n",
        "    FILE_NAME_CIFAR = \"cifar-10-python.tar.gz\"\n",
        "if cifar ==100:\n",
        "    FILE_NAME_CIFAR = \"cifar-100-python.tar.gz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83rOcvyHZkSt"
      },
      "source": [
        "Завантажимо дані"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYsnkkzoZj5x"
      },
      "outputs": [],
      "source": [
        "def load_and_unzip_cifar(url:str, file_name:str):\n",
        "  '''Завантаження і розпакування файлу-архіву, якщо ще цього не зроблено'''\n",
        "  from pathlib import Path\n",
        "  # Завантаження даних\n",
        "  full_url_to_load = url + file_name\n",
        "  print(full_url_to_load)\n",
        "  if Path(file_name).exists() and Path(file_name).is_file():\n",
        "    print(f\"Файл {file_name} вже є. Немає потреби його завантажувати!\")\n",
        "  else:\n",
        "    print(f\"Заватажуємо файл {file_name} \")\n",
        "    !wget $full_url_to_load\n",
        "  # Розпакування даних\n",
        "  # Створення робочої папки через stem i split - відсікання суфіксів\n",
        "  work_dir = Path(str((Path() / Path(file_name).stem)).split(\".\")[0])\n",
        "  # print(work_dir)\n",
        "  if work_dir.exists() and work_dir.is_dir():\n",
        "    print(f\"Директорія {work_dir} вже існує, перезаписуємо файли\")\n",
        "    # Перезаписуємо, бо є файли, які не дають видалити папку\n",
        "    !tar -xzvf $file_name\n",
        "  else:\n",
        "    print(f\"Розпакуємо файл {file_name}\")\n",
        "    !tar -xzvf $file_name\n",
        "\n",
        "load_and_unzip_cifar(URL_DIR, FILE_NAME_CIFAR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfYbv3eOcfbZ"
      },
      "outputs": [],
      "source": [
        "def unpickle(file):\n",
        "  '''Десеріалізація даних з pickle-файлу'''\n",
        "  import pickle\n",
        "  with open(file, 'rb') as fo:\n",
        "      dict = pickle.load(fo, encoding='bytes')\n",
        "  return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so6i2rKUEAq_"
      },
      "outputs": [],
      "source": [
        "if cifar == 10:\n",
        "  metadata_path = './cifar-10-batches-py/batches.meta' # шлях до даних\"\n",
        "  metadata = unpickle(metadata_path)\n",
        "  superclass_dict = dict(list(enumerate(metadata[b'label_names'])))\n",
        "if cifar ==100:\n",
        "  metadata_path = './cifar-100-python/meta' # шлях до даних\n",
        "  metadata = unpickle(metadata_path)\n",
        "  superclass_dict = dict(list(enumerate(metadata[b'coarse_label_names'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-dvkj6DXEgW"
      },
      "source": [
        "### Завантаження тренувальної та тестувальної вибірок (використовуючи суперкласи):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZzoTjKmXEgX"
      },
      "outputs": [],
      "source": [
        "if cifar == 100:\n",
        "  data_pre_path = './cifar-100-python/' # шлях до даних\n",
        "  # шляхи до файлів\n",
        "  data_train_path = data_pre_path + 'train'\n",
        "  data_test_path = data_pre_path + 'test'\n",
        "  # Зчитуємо словники\n",
        "  data_train_dict = unpickle(data_train_path)\n",
        "  data_test_dict = unpickle(data_test_path)\n",
        "  # Отримуємо дані (вибираємо coarse_labels щоб отримати всі 100 класів)\n",
        "  data_train = data_train_dict[b'data']\n",
        "  label_train = np.array(data_train_dict[b'coarse_labels'])\n",
        "  data_test = data_test_dict[b'data']\n",
        "  label_test = np.array(data_test_dict[b'coarse_labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhZmR_u9GXMv"
      },
      "source": [
        "### Завантаження тренувальної та тестувальної вибірок для Cifar-10:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlW_kO6P6DOx"
      },
      "outputs": [],
      "source": [
        "if cifar == 10:\n",
        "  data_pre_path = './cifar-10-batches-py/' # шлях до даних\n",
        "  data_train = None\n",
        "  label_train = []\n",
        "  data_test = None\n",
        "  label_test = []\n",
        "  # Тренувальні вибірки\n",
        "  for _ in range(1,6):\n",
        "    data_train_dict = unpickle(data_pre_path + f\"data_batch_{_}\")\n",
        "    if _ == 1:\n",
        "      data_train = data_train_dict[b'data']\n",
        "    else:\n",
        "      data_train = np.vstack((data_train, data_train_dict[b'data']))\n",
        "    label_train += data_train_dict[b'labels']\n",
        "  #  Тестувальні вибірки\n",
        "  data_test_dict = unpickle(data_pre_path + \"test_batch\")\n",
        "  data_test = data_test_dict[b'data']\n",
        "  label_test = data_test_dict[b'labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEejGejaXEgX"
      },
      "source": [
        "Поверхнево дослідимо дані"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM-kjaIEXEgY"
      },
      "outputs": [],
      "source": [
        "type(data_train.shape), type(data_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cZoWLveXEgZ"
      },
      "outputs": [],
      "source": [
        "data_train.shape, data_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q4hdxPtXEgZ"
      },
      "outputs": [],
      "source": [
        "np.info(data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfZN0XlfXEga"
      },
      "outputs": [],
      "source": [
        "data_train.reshape(len(data_train), 3, 32, 32).shape, data_test.reshape(len(data_test), 3, 32, 32).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FyEnk0rXEga"
      },
      "source": [
        "### Зміна розмірності зображень - виконувати лише раз у колабі!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V5Pd00UXEgb"
      },
      "outputs": [],
      "source": [
        "# Транспонується саме (0,2,3,1) - щоб отримати:\n",
        "# 0 - Позицію картинки\n",
        "# 2 - Значення висоти в пікселях\n",
        "# 3 - Значення ширини в пікселях\n",
        "# 1 - значення кольорів у RGB\n",
        "# Якщо набрати (0,3,2,1) - картинка перевертається на 90град - висота і ширина міняються місцями\n",
        "\n",
        "data_train = data_train.reshape(len(data_train), 3, 32, 32).transpose(0,2,3,1)\n",
        "data_test = data_test.reshape(len(data_test), 3, 32, 32).transpose(0,2,3,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d883O9sXEgb"
      },
      "outputs": [],
      "source": [
        "np.info(data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_UW15r2XEgc"
      },
      "outputs": [],
      "source": [
        "_ = random.randint(0,len(data_train))\n",
        "print(f\"For picture #{_} - {label_train[_]} ({superclass_dict[label_train[_]]}):\")\n",
        "fig = plt.figure(figsize=(1,1))\n",
        "fig.add_subplot(1,1,1)\n",
        "plt.imshow(data_train[_])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYDz98DSXEgc"
      },
      "source": [
        "Трохи більше картинок з тренувальної та тестувальної вибірок..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "391BTDRWXEgd"
      },
      "outputs": [],
      "source": [
        "def show_pictures_n_m(data,n,m:int):\n",
        "    fig = plt.figure(figsize=(max(n,m),max(n,m)))\n",
        "    for _ in range (n*m):\n",
        "        fig.add_subplot(n,m,_+1)\n",
        "        plt.imshow(data[_])\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "show_pictures_n_m(data_train,5,5)\n",
        "show_pictures_n_m(data_test,3,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymUlMd5jXEgd"
      },
      "source": [
        "### Подальша підготовка даних для тренувань мережі"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcC7aoOL9AST"
      },
      "outputs": [],
      "source": [
        "data_train.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BUFFER_SIZE = data_train.shape[0]\n",
        "BATCH_SIZE = 256\n",
        "# як альтернатива\n",
        "# BATCH_SIZE = int(BUFFER_SIZE / 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKoJ_NlOMTTG"
      },
      "outputs": [],
      "source": [
        "def prepare_images(data_set, buffer_size, batch_size):\n",
        "  prepared_images = data_set.astype(\"float32\")\n",
        "  prepared_images = (prepared_images - 127.5) / 127.5\n",
        "  # prepared_images = prepared_images[:1000]\n",
        "  # prepared_images = tf.data.Dataset.from_tensor_slices(prepared_images)\n",
        "  # Перемішаємо дані випадковим чином\n",
        "  # prepared_images = prepared_images.shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)\n",
        "  return prepared_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvqHdZkKF4tJ"
      },
      "outputs": [],
      "source": [
        "train_images = prepare_images(data_train, BUFFER_SIZE, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK345-ImHs2q"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8Iw9_YDFeNW"
      },
      "outputs": [],
      "source": [
        "len(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BUFFER_SIZE = train_images.shape[0]\n",
        "BATCH_SIZE = int(BUFFER_SIZE / 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fvt4flRJIJm"
      },
      "source": [
        "# Створення та тренування генератора і дискримінатора"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwRuLt3jURls"
      },
      "source": [
        "Генератор - використовує шари `tf.keras.layers.Conv2DTranspose` (для підвищення дискретності),щоб створювати зображення з вихідного випадкового шуму (random noise). Починається шаром `Dense`, який приймає це початкове значення, а потім кілька разів підвищує дискретизацію, поки не досягне бажаного розміру зображення."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwcfTZVuVNsv"
      },
      "source": [
        "Дискримінатор - Це класифікатор зображень на основі CNN.\n",
        "\n",
        "Він класифікує (визначає) якою є згенерована картинка - справжньою чи фейковою. Модель тренується показувати позитивне значення для справжньої картинки і негативне для фейкової."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kryrUjTcWQM4"
      },
      "outputs": [],
      "source": [
        "def real_or_fake(decision):\n",
        "  if decision>=0:\n",
        "    return \"Real\"\n",
        "  else:\n",
        "    return \"Fake\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeEGokG1RpFA"
      },
      "source": [
        "## Змінні"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5S91_euRibv"
      },
      "outputs": [],
      "source": [
        "# Швидкість навчання\n",
        "LEARNING_RATE = 2e-4\n",
        "# Вхідна розмірність\n",
        "INPUT_DIM = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9k7fn7CIURo"
      },
      "source": [
        "## Функції створення генератора та дискримінатора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXFkDrjuJePj"
      },
      "outputs": [],
      "source": [
        "def build_generator(nodes=4, input_dim=100, alpha = 0.2):\n",
        "  \"\"\"Створення генератора\"\"\"\n",
        "  model = tf.keras.Sequential()\n",
        "  # Основа для зображення 4 * 4 * 256\n",
        "  n_f_nodes = 256 * nodes * nodes\n",
        "  model.add(layers.Dense(n_f_nodes, input_dim = input_dim))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "  model.add(layers.Reshape((nodes, nodes, 256)))\n",
        "  # Підвищення дискретизації до 8x8\n",
        "  model.add(layers.Conv2DTranspose(128, (nodes, nodes), strides=(2, 2), padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "  # Підвищення дискретизації до 16x16\n",
        "  model.add(layers.Conv2DTranspose(128, (nodes, nodes), strides=(2, 2), padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "  # Підвищення дискретизації до 32x32\n",
        "  model.add(layers.Conv2DTranspose(128, (nodes, nodes), strides=(2, 2), padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "  # Вихідний шар\n",
        "  model.add(layers.Conv2D(3, (3, 3), activation='tanh', padding='same'))\n",
        "\n",
        "  print(\"Створено Генератор\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6is49HB2-9i"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(input_shape = (32,32,3), alpha = 0.2):\n",
        "  \"\"\"Створення дискримінатора\"\"\"\n",
        "  model = tf.keras.Sequential()\n",
        "  # Перший шар\n",
        "  model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n",
        "  model.add(layers.LeakyReLU(alpha=alpha))\n",
        "  # model.add(layers.Dropout(0.3))\n",
        "  # 3меншення дискретизації\n",
        "  model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  # model.add(layers.Dropout(0.3))\n",
        "  # 3меншення дискретизації\n",
        "  model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  # model.add(layers.Dropout(0.3))\n",
        "  # 3меншення дискретизації\n",
        "  model.add(layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same'))\n",
        "  model.add(layers.LeakyReLU(alpha=alpha))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "  # Класифікатор\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  print(\"Створено Дискримінатор\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWjtgKKY3qIG"
      },
      "outputs": [],
      "source": [
        "def build_gen_disc_model(gen_model, disc_model):\n",
        "  '''\n",
        "  Комбінована модель генератора і дискримінатора,\n",
        "  для оновлення генератора\n",
        "  '''\n",
        "  # зробимо ваги для дискримінатора нетренованими\n",
        "  disc_model.trainable = False\n",
        "  # З'єднаємо дві моделі\n",
        "  model = tf.keras.Sequential()\n",
        "  # Додамо генератор\n",
        "  model.add(gen_model)\n",
        "  # Додамо дискримінатор\n",
        "  model.add(disc_model)\n",
        "\n",
        "  print(\"Створено Генератор-Дискримінатор\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA-717jD3FtB"
      },
      "outputs": [],
      "source": [
        "def compile_model(model, opt, loss, metrics):\n",
        "  '''Компіляція створеної моделі'''\n",
        "  model.compile(loss = loss, optimizer = opt, metrics = metrics)\n",
        "  print(\"Модель скомпільовано\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9UmZ8yP8_DK"
      },
      "source": [
        "## Функції роботи генератора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXOz5oQtJ8Wn"
      },
      "outputs": [],
      "source": [
        "def generate_real_images(dataset, n_images):\n",
        "  '''Генерація реальних зображень'''\n",
        "  # Виберемо випадкове зображення\n",
        "  idx = np.random.randint(0, dataset.shape[0], n_images)\n",
        "  # print(f\"Func = g_r_i, idx = {idx}\")\n",
        "  # print(f\"Func = g_r_i, idx.shape = {idx.shape}\")\n",
        "  # Отримаємо вибрані зображення\n",
        "  X = dataset[idx]\n",
        "  # print(f\"Func = g_r_i, X = {X}\")\n",
        "  # print(f\"Func = g_r_i, X.shape = {X.shape}\")\n",
        "  # Згенеруємо 'справжні' мітки класу - одинична матриця\n",
        "  y = np.ones((n_images, 1))\n",
        "  # print(f\"Func = g_r_i, y = {y}\")\n",
        "  # print(f\"Func = g_r_i, y.shape = {y.shape}\")\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HjMwop4oBPE"
      },
      "outputs": [],
      "source": [
        "def generate_points(input_dim, n_images):\n",
        "  '''\n",
        "  Генерація точок в прихованому вхідному шарі просторі\n",
        "  як вхідні дані для генератора\n",
        "  '''\n",
        "  # Генерація точок\n",
        "  x_input = np.random.randn(input_dim * n_images)\n",
        "  # print(f\"Func = g_p, x_input.shape = {x_input.shape}\")\n",
        "  # Зміна форми до форми пакету виходів мережі\n",
        "  x_input = x_input.reshape(n_images, input_dim)\n",
        "  # print(f\"Func = g_p, x_input.reshape = {x_input.shape}\")\n",
        "  return x_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZJmX2oaoEIB"
      },
      "outputs": [],
      "source": [
        "def generate_fake_images(gen_model, input_dim, n_images):\n",
        "  '''Генерація фейкових зображень'''\n",
        "  # Генеруємо точки для вхідного шару\n",
        "  x_input = generate_points(input_dim, n_images)\n",
        "  # print(f\"Func = g_f_i, x_input.shape = {x_input.shape}\")\n",
        "  # Генерація (передбачення) генератором\n",
        "  X = gen_model.predict(x_input, verbose = 0)\n",
        "  # print(f\"Func = g_f_i, X_input.shape = {X.shape}\")\n",
        "  # Створення 'фейкового зображення' - матриця нулів\n",
        "  y = np.zeros((n_images, 1))\n",
        "  # print(f\"Func = g_f_i, y.shape = {y.shape})\")\n",
        "  return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmM1BGQM9GC9"
      },
      "source": [
        "## Функції обробки результатів роботи моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_tbOakB87Ny"
      },
      "outputs": [],
      "source": [
        "def show_save_plot(examples, epoch, to_save, n=5):\n",
        "  '''\n",
        "  Створення, вивіт та за необхідності збереження зображень\n",
        "  '''\n",
        "  # Масштабування виводу зображення (від [-1,1] до [0,1])\n",
        "  examples = (examples + 1) / 2.0\n",
        "  # Малювання зображень\n",
        "  for i in range(n * n):\n",
        "      plt.subplot(n, n, 1 + i)\n",
        "      # Приберемо виведення осей\n",
        "      plt.axis('off')\n",
        "      # Вивід зображення\n",
        "      plt.imshow(examples[i])\n",
        "  # Збереження зображення за необхідності\n",
        "  if to_save:\n",
        "    filename = 'gen_img_e%03d.png' % (epoch+1)\n",
        "    plt.savefig(filename)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9YlOaF6_iBs"
      },
      "outputs": [],
      "source": [
        "def summary_performance(epoch, gen_model, disc_model, dataset, input_dim, n_images=100, to_save = False):\n",
        "  '''\n",
        "  Ф-я оцінки дискримінатором, побудова зображень\n",
        "  '''\n",
        "  # Генерація реальних зображень\n",
        "  # print(\"Func =s_p_i, Генерація реальних зображень\")\n",
        "  X_real, y_real = generate_real_images(dataset, n_images)\n",
        "  # Оцінка дискримінатором згенерованих реальних зображень\n",
        "  # print(\"Func =s_p_i, Оцінка дискримінатором згенерованих реальних зображень\")\n",
        "  _, accuracy_real = disc_model.evaluate(X_real, y_real, verbose=0)\n",
        "  # Генерація фейкових зображень\n",
        "  # print(\"Func =s_p_i, Генерація фейкових зображень\")\n",
        "  x_fake, y_fake = generate_fake_images(gen_model, input_dim, n_images)\n",
        "  # Оцінка дискримінатором згенерованих фейкових зображень\n",
        "  # print(\"Func =s_p_i, Оцінка дискримінатором згенерованих фейкових зображень\")\n",
        "  _, accuracy_fake = disc_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "  # Загальний вивід інформації роботи дискримінатора\n",
        "  # print(\"Func =s_p_i, Загальний вивід інформації роботи дискримінатора\")\n",
        "  print(f\"Точність для реального зображення: {accuracy_real*100 : .0f}\\n\\\n",
        "Точність для фейкового зображення {accuracy_fake * 100:.0f}\")\n",
        "  show_save_plot(x_fake, epoch, to_save = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylFi6e4SFkOI"
      },
      "source": [
        "## Функція тренування генератора і дискримінатора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHR7-eXoFyVX"
      },
      "outputs": [],
      "source": [
        "def train(gen_model, disc_model, gen_disc_model, dataset, input_dim, n_epochs, batch_size):\n",
        "  '''Функція тренування генератора і дискримінатора'''\n",
        "  from IPython import display\n",
        "  import time as tm\n",
        "  start = tm.time()\n",
        "  # Кількість пакетів на епоху\n",
        "  batch_per_epoch = int(dataset.shape[0] / batch_size)\n",
        "  half_batch = int(batch_size / 2)\n",
        "  # Ручний перелік епох\n",
        "  for i in range(n_epochs):\n",
        "    ep_start = tm.time()\n",
        "    print(f\"Епоха {i+1} з {n_epochs}\")\n",
        "    # Перелік пакетів у тренувальній вибірці\n",
        "    for j in range(batch_per_epoch):\n",
        "      print(f\"Ep={i}, J={j+1} з {batch_per_epoch}\")\n",
        "      # Випадкова генерація 'реальних' зображень\n",
        "      # print(\"Випадкова генерація 'реальних' зображень\")\n",
        "      X_real, y_real = generate_real_images(dataset, half_batch)\n",
        "      # Оновлення вагів дискримінатора для 'реальних' зображень\n",
        "      # print(\"Оновлення вагів дискримінатора для 'реальних' зображень\")\n",
        "      d_loss1, _ = disc_model.train_on_batch(X_real, y_real)\n",
        "      # Генерація 'фейкових' зображень\n",
        "      # print(\"Генерація 'фейкових' зображень\")\n",
        "      X_fake, y_fake = generate_fake_images(gen_model, input_dim, half_batch)\n",
        "      # Оновлення вагів дискримінатора для 'фейкових' зображень\n",
        "      # print(\"Оновлення вагів дискримінатора для 'фейкових' зображень\")\n",
        "      d_loss2, _ = disc_model.train_on_batch(X_fake, y_fake)\n",
        "      # Підготовка точок у вхідному шарі як вхідних для генератора\n",
        "      # print(\"Підготовка точок у вхідному шарі як вхідних для генератора\")\n",
        "      X_gen_disc = generate_points(input_dim, batch_size)\n",
        "      # Створення інвертованих міток для фейкових зображень\n",
        "      # print(\"Створення інвертованих міток для фейкових зображень\")\n",
        "      y_gen_disc = np.ones((batch_size, 1))\n",
        "      # Оновлення генератора через значення помилок дискримінатора\n",
        "      # print(\"Оновлення генератора через значення помилок дискримінатора\")\n",
        "      gen_disc_loss = gen_disc_model.train_on_batch(X_gen_disc, y_gen_disc)\n",
        "      # Загальна втрата для цього пакету\n",
        "      if j+1 % 100 == 0:\n",
        "          print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i + 1, j + 1, batch_per_epoch, d_loss1, d_loss2, gen_disc_loss))\n",
        "    # Проміжна оцінка продуктивності моделі\n",
        "    if (i+1) % 1 == 0:\n",
        "      summary_performance(i, gen_model, disc_model, dataset, input_dim, to_save=False)\n",
        "    print(f\"Час навчання епохи {i+1} - {(tm.time()-ep_start):.2f} сек\")\n",
        "  print(f\"Загальний час навчання на {n_epochs} епохах - {(tm.time()-start):.2f} сек\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gXtRH3-Nj0X"
      },
      "source": [
        "## Створення і налаштування генератора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5d5_2SfNjbu"
      },
      "outputs": [],
      "source": [
        "LOSS = \"binary_crossentropy\"\n",
        "OPT = Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = []\n",
        "\n",
        "generator = build_generator(nodes=4, input_dim=100, alpha = 0.2)\n",
        "compile_model(generator, OPT, LOSS, METRICS)\n",
        "\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCgnvVXdPlnX"
      },
      "source": [
        "## Створення і налаштування дискримінатора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4ZWT0hEPs4Y"
      },
      "outputs": [],
      "source": [
        "LOSS = \"binary_crossentropy\"\n",
        "OPT = Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = [\"accuracy\"]\n",
        "\n",
        "discriminator = build_discriminator(input_shape = (32,32,3), alpha = 0.2)\n",
        "compile_model(discriminator, OPT, LOSS, METRICS)\n",
        "\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll9Hp6zyKB0J"
      },
      "source": [
        "## Створення і налаштування загальної моделі генератор-дискримінатор"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t30xwzwUKCbE"
      },
      "outputs": [],
      "source": [
        "LOSS = \"binary_crossentropy\"\n",
        "OPT = Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = []\n",
        "\n",
        "gen_disc = build_gen_disc_model(generator, discriminator)\n",
        "compile_model(gen_disc, OPT, LOSS, METRICS)\n",
        "\n",
        "gen_disc.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTJlAVSSTVEa"
      },
      "outputs": [],
      "source": [
        "# Генерування зображення та його класифікація дискримінатором без навчання\n",
        "noise = tf.random.normal([1, INPUT_DIM])\n",
        "generated_image = generator(noise, training = False)\n",
        "decision = discriminator(generated_image).numpy()[0][0]\n",
        "print(f\"generated_image.shape = {generated_image.shape}\") #TensorShape([1, 32, 32, 3])\n",
        "print(f\"Точність decision = {decision}\")\n",
        "verdict = real_or_fake(decision)\n",
        "print(f\"Зображення - {verdict}\")\n",
        "plt.imshow(generated_image[generated_image.shape[0]-1, :,: ,0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E5SP3D6Oelj"
      },
      "source": [
        "## Тренування моделей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyE-vJfYrKzn"
      },
      "source": [
        "Задаємо тренувальні параметри"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pVJs0bNrX7r"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zucinuqQOOe"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sprITcbXQTeT"
      },
      "outputs": [],
      "source": [
        "type(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hHdkMhTr2NO"
      },
      "outputs": [],
      "source": [
        "train(gen_model = generator,\n",
        "      disc_model = discriminator,\n",
        "      gen_disc_model = gen_disc,\n",
        "      dataset = train_images,\n",
        "      input_dim = INPUT_DIM,\n",
        "      n_epochs = EPOCHS,\n",
        "      batch_size = BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator.save(\"generator.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "discriminator.save(\"discriminator.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
